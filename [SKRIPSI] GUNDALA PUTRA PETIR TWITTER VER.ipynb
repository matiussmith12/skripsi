{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import numpy as np # linear algebra\n",
    "import os\n",
    "import re \n",
    "import string\n",
    "import emoji\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset csv\n",
    "dataset = pd.read_csv(\"Gundala6k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-17 14:24:36+00:00</td>\n",
       "      <td>BerisikRadio</td>\n",
       "      <td>#interview bareng Compadres gimana akhirnya te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-17 12:02:00+00:00</td>\n",
       "      <td>channelmaenan</td>\n",
       "      <td>GUNDALA ARMORED SUIT (Upgrade) 1:6 statue Limi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-15 14:14:08+00:00</td>\n",
       "      <td>Fow_Lagih</td>\n",
       "      <td>kirain rame.. #gundala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-14 12:08:07+00:00</td>\n",
       "      <td>roul_am</td>\n",
       "      <td>Nice combat with martial arts #gundala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-14 06:51:18+00:00</td>\n",
       "      <td>bapakranger</td>\n",
       "      <td>@Bumilangit Corner @GandariaCity mall L2 Skuy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No                   Datetime           User  \\\n",
       "0  0  2020-02-17 14:24:36+00:00   BerisikRadio   \n",
       "1  1  2020-02-17 12:02:00+00:00  channelmaenan   \n",
       "2  2  2020-02-15 14:14:08+00:00      Fow_Lagih   \n",
       "3  3  2020-02-14 12:08:07+00:00        roul_am   \n",
       "4  4  2020-02-14 06:51:18+00:00    bapakranger   \n",
       "\n",
       "                                                Text  \n",
       "0  #interview bareng Compadres gimana akhirnya te...  \n",
       "1  GUNDALA ARMORED SUIT (Upgrade) 1:6 statue Limi...  \n",
       "2                             kirain rame.. #gundala  \n",
       "3             Nice combat with martial arts #gundala  \n",
       "4  @Bumilangit Corner @GandariaCity mall L2 Skuy ...  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Text'] = dataset['Text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(592, ':/'),\n",
       " (50, ':('),\n",
       " (33, ':)'),\n",
       " (12, \":')\"),\n",
       " (10, ':D'),\n",
       " (5, ':3'),\n",
       " (3, ':|'),\n",
       " (3, ':p'),\n",
       " (3, \":'v\"),\n",
       " (2, ':v'),\n",
       " (2, ':o'),\n",
       " (1, \":'D\"),\n",
       " (1, \":'9\")]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cari emoticon yang dipake di dataset ini\n",
    "tweets_text = dataset.Text.str.cat()\n",
    "emos = set(re.findall(r\" ([xX:;][-*']?.) \",tweets_text))\n",
    "emos_count = []\n",
    "for emo in emos:\n",
    "    emos_count.append((tweets_text.count(emo), emo))\n",
    "sorted(emos_count, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy emoticons: {':p', \":'v\", ':D', ':)', \":'D\", \":')\", ':3', ':v'}\n",
      "Sad emoticons: {':(', ':|', ':/'}\n"
     ]
    }
   ],
   "source": [
    "POS_EMO = r\" ([xX;:]'?[dD)]|:'?[\\)]|[;:][pP]|[:]'?[v3]) \"\n",
    "NEG_EMO = r\" (:'?[\\/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(POS_EMO, tweets_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(NEG_EMO, tweets_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset['Text'] != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING TEXT : Data Cleaning\n",
    "def clean_tweets(tweet):    \n",
    "    # remove link web\n",
    "    tweet = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\", \"\", tweet)\n",
    "    # remove link pic twitter\n",
    "    tweet = re.sub(r\"(pic\\.twitter\\.com\\/[a-zA-Z0-9][a-zA-Z0-9]+)\", \"\", tweet)\n",
    "    # remove username\n",
    "    tweet = re.sub(r\"@[^\\s]+[\\s]?\", '', tweet)\n",
    "    # remove 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', '.', tweet)\n",
    "    # remove numbers \n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    # HARUS DILAKUKAN: HAPUS HASHTAG SELAIN #GUNDALA #gUNDALA #Gundala \n",
    "    # tweet = re.sub(r'#', '', tweet)\n",
    "    #tweet = re.sub(r'(?<=#)\\w+', '', tweet)\n",
    "    # remove other characters \n",
    "    tweet = re.sub(r'[^ -_:?!.a-z0-9]', '', tweet)\n",
    "    # replace multiple spaces with a single space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    # casefolding\n",
    "    tweet = tweet.lower()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function preprocessing text to the dataset column [Text]\n",
    "dataset['Clean Tweets'] = dataset['Text'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SPLITTING\n",
    "raw_sentences = [sent_tokenize(i) for i in dataset['Clean Tweets']]\n",
    "dataset['Sentence Splitting'] = raw_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD TOKENIZATION \n",
    "def tokenization_w(raw_s): # fungsi untuk tokenisasi\n",
    "    #w_new = []\n",
    "    all = [] # buat list baru untuk tampung semuanya \n",
    "    for s in raw_s: # untuk semua s di dalam tweet yang sudah di-sentence splitting \n",
    "        w_new = [] # buat variable untuk tampung semuanya \n",
    "        for w in s: # untuk semua kalimat yang ada di tweet\n",
    "            w_token = word_tokenize(w) # di tokenize \n",
    "            w_new.append(w_token) # \n",
    "        all.append(w_new)\n",
    "    return all\n",
    "\n",
    "clean_words = tokenization_w(raw_sentences)\n",
    "dataset['Tweet Tokens'] = clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "list_stopwords = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAMBAH STOPWORDS\n",
    "list_stopwords.extend([\"yg\", \"dgn\", \"ny\", \"kalo\", \"klo\", \"kl\", \"biar\", \"bikin\", \n",
    "                       \"bilang\", \"krn\", \"nya\", \"nih\", \"sih\",\"jgn\",\"sdh\", \"aja\",\n",
    "                      \"hehe\", \"haha\", \"loh\", \"ya\", \"bah\"])\n",
    "list_stopwords.remove(\"sangat\")\n",
    "list_stopwords.remove(\"sekali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASUKIN KAMUS KE DATAFRAME\n",
    "txt_stopwords = pd.read_csv(\"stopword_list_tala.txt\", names=[\"stopwords\"], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopwords[\"stopwords\"][0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to dictionary \n",
    "list_stopwords = set(list_stopwords)\n",
    "# list_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOPWORDS REMOVER \n",
    "def stopwords_w(kalimat):\n",
    "    all = []\n",
    "    for item in kalimat:\n",
    "        temp = []\n",
    "        for tweet in item:\n",
    "            if tweet not in list_stopwords:\n",
    "                temp.append(tweet)\n",
    "        all.append(temp)\n",
    "    return all \n",
    "\n",
    "dataset['Tweets Stopwords'] = dataset['Tweet Tokens'].apply(stopwords_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.create_stop_word_remover()\n",
    "# stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEMMING \n",
    "#def stemmer_w(kalimat):\n",
    "    #all = []\n",
    "    #for item in kalimat:\n",
    "        #temp = []\n",
    "        #for tweet in item:\n",
    "            #stem_words = stemmer.stem(tweet) # di tokenize \n",
    "            #temp.append(tweet)\n",
    "        #all.append(temp)\n",
    "    #return all \n",
    "\n",
    "#dataset['Hasil Stemming'] = dataset['Stops'].apply(stemmer_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '#gundala.',\n",
       " 'walaupun masih banyak kekurangan sana sini tapi sebagai pembuka adiwira indonesia effortnya dan tim patut diacungi jempol']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Sentence Splitting'][338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aku ga ngerti semua pemainnya bagus.',\n",
       " 'bahkan yg cuma cameo doang aja aku suka.',\n",
       " 'nangis pas denger backsoundnya karena sekeren itu #gundala']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Sentence Splitting'][4526]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keren banget ini harus nonton lagi dan gak sabar nunggu lanjutannya #gundala #pinkhodie']"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Sentence Splitting'][4349]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aku ga ngerti semua pemainnya bagus. Bahkan yg cuma cameo doang aja aku suka. Nangis pas denger backsoundnya karena sekeren itu #Gundala'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Text'][4526]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aku ga ngerti semua pemainnya bagus.',\n",
       " 'bahkan yg cuma cameo doang aja aku suka.',\n",
       " 'nangis pas denger backsoundnya karena sekeren itu #gundala']"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Sentence Splitting'][4526]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aku', 'ga', 'ngerti', 'semua', 'pemainnya', 'bagus', '.'],\n",
       " ['bahkan', 'yg', 'cuma', 'cameo', 'doang', 'aja', 'aku', 'suka', '.'],\n",
       " ['nangis',\n",
       "  'pas',\n",
       "  'denger',\n",
       "  'backsoundnya',\n",
       "  'karena',\n",
       "  'sekeren',\n",
       "  'itu',\n",
       "  '#',\n",
       "  'gundala']]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweet Tokens'][4526]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ga', 'ngerti', 'pemainnya', 'bagus', '.'],\n",
       " ['cameo', 'doang', 'suka', '.'],\n",
       " ['nangis', 'pas', 'denger', 'backsoundnya', 'sekeren', '#', 'gundala']]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweets Stopwords'][4526]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#',\n",
       "  'gundala',\n",
       "  'sempet',\n",
       "  'gamau',\n",
       "  'nonton',\n",
       "  'tp',\n",
       "  'akhirnya',\n",
       "  'jd',\n",
       "  'nonton',\n",
       "  'wkwk',\n",
       "  'dan',\n",
       "  'apa',\n",
       "  '?'],\n",
       " ['suka', 'bangettt', '!'],\n",
       " ['gue',\n",
       "  'fikir',\n",
       "  'dih',\n",
       "  'jd',\n",
       "  'superhero',\n",
       "  'pasti',\n",
       "  'cringe',\n",
       "  'bgt',\n",
       "  'tau',\n",
       "  'nya',\n",
       "  'ngga',\n",
       "  'malah',\n",
       "  'keren']]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweet Tokens'][2133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#',\n",
       "  'gundala',\n",
       "  'sempet',\n",
       "  'gamau',\n",
       "  'nonton',\n",
       "  'tp',\n",
       "  'jd',\n",
       "  'nonton',\n",
       "  'wkwk',\n",
       "  '?'],\n",
       " ['suka', 'bangettt', '!'],\n",
       " ['gue',\n",
       "  'fikir',\n",
       "  'dih',\n",
       "  'jd',\n",
       "  'superhero',\n",
       "  'cringe',\n",
       "  'bgt',\n",
       "  'tau',\n",
       "  'ngga',\n",
       "  'keren']]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweets Stopwords'][2133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akhirnya loh nonton #gundala!!!!!',\n",
       " 'tidak sempurna tapi epic banget sih warbyasak.',\n",
       " 'cara memperkenalkan karakter-karakternya juga oke (hint: anak bapak).',\n",
       " 'gue suka banget jokesnya banyak yang kena juga lawak bat gue antusias banget menyambut kelanjutan bumi langit universe ini.']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Sentence Splitting'][2441]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['akhirnya', 'loh', 'nonton', '#', 'gundala', '!', '!', '!', '!', '!'],\n",
       " ['tidak', 'sempurna', 'tapi', 'epic', 'banget', 'sih', 'warbyasak', '.'],\n",
       " ['cara',\n",
       "  'memperkenalkan',\n",
       "  'karakter-karakternya',\n",
       "  'juga',\n",
       "  'oke',\n",
       "  '(',\n",
       "  'hint',\n",
       "  ':',\n",
       "  'anak',\n",
       "  'bapak',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['gue',\n",
       "  'suka',\n",
       "  'banget',\n",
       "  'jokesnya',\n",
       "  'banyak',\n",
       "  'yang',\n",
       "  'kena',\n",
       "  'juga',\n",
       "  'lawak',\n",
       "  'bat',\n",
       "  'gue',\n",
       "  'antusias',\n",
       "  'banget',\n",
       "  'menyambut',\n",
       "  'kelanjutan',\n",
       "  'bumi',\n",
       "  'langit',\n",
       "  'universe',\n",
       "  'ini',\n",
       "  '.']]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweet Tokens'][2441]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nonton', '#', 'gundala', '!', '!', '!', '!', '!'],\n",
       " ['sempurna', 'epic', 'banget', 'warbyasak', '.'],\n",
       " ['memperkenalkan',\n",
       "  'karakter-karakternya',\n",
       "  'oke',\n",
       "  '(',\n",
       "  'hint',\n",
       "  ':',\n",
       "  'anak',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['gue',\n",
       "  'suka',\n",
       "  'banget',\n",
       "  'jokesnya',\n",
       "  'kena',\n",
       "  'lawak',\n",
       "  'bat',\n",
       "  'gue',\n",
       "  'antusias',\n",
       "  'banget',\n",
       "  'menyambut',\n",
       "  'kelanjutan',\n",
       "  'bumi',\n",
       "  'langit',\n",
       "  'universe',\n",
       "  '.']]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweets Stopwords'][2441]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#gundala keren!!!!!!',\n",
       " 'awalan yg bagus buat bumilangit cinematic universe.',\n",
       " 'gw percaya ini bakal sebooming dan gak kalah keren sama mcu!']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Sentence Splitting'][2916]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#', 'gundala', 'keren', '!', '!', '!', '!', '!', '!'],\n",
       " ['awalan', 'yg', 'bagus', 'buat', 'bumilangit', 'cinematic', 'universe', '.'],\n",
       " ['gw',\n",
       "  'percaya',\n",
       "  'ini',\n",
       "  'bakal',\n",
       "  'sebooming',\n",
       "  'dan',\n",
       "  'gak',\n",
       "  'kalah',\n",
       "  'keren',\n",
       "  'sama',\n",
       "  'mcu',\n",
       "  '!']]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweet Tokens'][2916]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Akhirnya loh nonton #Gundala!!!!! Tidak sempurna tapi epic banget sih warbyasak. Cara memperkenalkan karakter-karakternya juga oke (hint: anak Bapak). Gue suka banget jokesnya banyak yang kena juga lawak bat Gue antusias banget menyambut kelanjutan Bumi Langit Universe ini. '"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Text'][2441]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'akhirnya loh nonton #gundala!!!!! tidak sempurna tapi epic banget sih warbyasak. cara memperkenalkan karakter-karakternya juga oke (hint: anak bapak). gue suka banget jokesnya banyak yang kena juga lawak bat gue antusias banget menyambut kelanjutan bumi langit universe ini. '"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Clean Tweets'][2441]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.'],\n",
       " ['#', 'gundala', '.'],\n",
       " ['walaupun',\n",
       "  'masih',\n",
       "  'banyak',\n",
       "  'kekurangan',\n",
       "  'sana',\n",
       "  'sini',\n",
       "  'tapi',\n",
       "  'sebagai',\n",
       "  'pembuka',\n",
       "  'adiwira',\n",
       "  'indonesia',\n",
       "  'effortnya',\n",
       "  'dan',\n",
       "  'tim',\n",
       "  'patut',\n",
       "  'diacungi',\n",
       "  'jempol']]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweet Tokens'][338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.'],\n",
       " ['#', 'gundala', '.'],\n",
       " ['kekurangan',\n",
       "  'pembuka',\n",
       "  'adiwira',\n",
       "  'indonesia',\n",
       "  'effortnya',\n",
       "  'tim',\n",
       "  'patut',\n",
       "  'diacungi',\n",
       "  'jempol']]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweets Stopwords'][338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['overall',\n",
       "  'sinematografinya',\n",
       "  'sangat',\n",
       "  'bagus',\n",
       "  'tweaks',\n",
       "  'here',\n",
       "  'and',\n",
       "  'there',\n",
       "  'but',\n",
       "  'still',\n",
       "  'i',\n",
       "  \"'m\",\n",
       "  'proud',\n",
       "  'of',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'and',\n",
       "  'i',\n",
       "  'love',\n",
       "  'it',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'first',\n",
       "  'step',\n",
       "  '.'],\n",
       " ['semoga',\n",
       "  'menerima',\n",
       "  'kritik',\n",
       "  'and',\n",
       "  'use',\n",
       "  'them',\n",
       "  'to',\n",
       "  'be',\n",
       "  'even',\n",
       "  'better',\n",
       "  'semangatt',\n",
       "  '!'],\n",
       " ['#', 'gundala']]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweets Stopwords'][5543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['very', 'quotable', '#', 'gundala', 'ni', '.'],\n",
       " ['suka',\n",
       "  'sangat',\n",
       "  'kata-kata',\n",
       "  'agung',\n",
       "  'ga',\n",
       "  'gunanya',\n",
       "  'hidup',\n",
       "  'ga',\n",
       "  'peduli',\n",
       "  'mikirin',\n",
       "  '!'],\n",
       " ['menusuk', 'jiwa', '.']]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Tweets Stopwords'][515]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''],\n",
       " ['', 'gundala', ''],\n",
       " ['kurang',\n",
       "  'buka',\n",
       "  'adiwira',\n",
       "  'indonesia',\n",
       "  'effortnya',\n",
       "  'tim',\n",
       "  'patut',\n",
       "  'acung',\n",
       "  'jempol']]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Hasil Stemming'][338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEMMING \n",
    "def stemmer_w(kalimat):\n",
    "    all = []\n",
    "    for item in kalimat:\n",
    "        temp = []\n",
    "        for tweet in item:\n",
    "            stem_words = stemmer.stem(tweet) # di tokenize \n",
    "            temp.append(stem_words)\n",
    "        all.append(temp)\n",
    "    return all \n",
    "\n",
    "dataset['Hasil Stemming'] = dataset['Tweets Stopwords'].apply(stemmer_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gokil', 'gundala', 'bagus', 'bangetttt', '', ''],\n",
       " ['koreo',\n",
       "  'antem',\n",
       "  'keren',\n",
       "  'banget',\n",
       "  'adegan',\n",
       "  'sadis',\n",
       "  'ngeri',\n",
       "  'main',\n",
       "  'manteb',\n",
       "  'tp',\n",
       "  'adegan',\n",
       "  'tbtb',\n",
       "  'ubah',\n",
       "  'jd',\n",
       "  'film',\n",
       "  'horror',\n",
       "  'wkwk',\n",
       "  '',\n",
       "  'gundala']]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Hasil Stemming'][2735]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  'gundala',\n",
       "  'sempet',\n",
       "  'gamau',\n",
       "  'nonton',\n",
       "  'tp',\n",
       "  'jd',\n",
       "  'nonton',\n",
       "  'wkwk',\n",
       "  ''],\n",
       " ['suka', 'bangettt', ''],\n",
       " ['gue',\n",
       "  'fikir',\n",
       "  'dih',\n",
       "  'jd',\n",
       "  'superhero',\n",
       "  'cringe',\n",
       "  'bgt',\n",
       "  'tau',\n",
       "  'ngga',\n",
       "  'keren']]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Hasil Stemming'][2133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nonton', '', 'gundala', '', '', '', '', ''],\n",
       " ['sempurna', 'epic', 'banget', 'warbyasak', ''],\n",
       " ['kenal', 'karakter', 'oke', '', 'hint', '', 'anak', '', ''],\n",
       " ['gue',\n",
       "  'suka',\n",
       "  'banget',\n",
       "  'jokesnya',\n",
       "  'kena',\n",
       "  'lawak',\n",
       "  'bat',\n",
       "  'gue',\n",
       "  'antusias',\n",
       "  'banget',\n",
       "  'sambut',\n",
       "  'lanjut',\n",
       "  'bumi',\n",
       "  'langit',\n",
       "  'universe',\n",
       "  '']]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Hasil Stemming'][2441]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
